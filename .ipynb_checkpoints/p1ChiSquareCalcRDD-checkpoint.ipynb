{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIC2020 - A2.1 Calculation of 200 most discriminative terms per category for amazon review dataset with Spark and Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://c100.local:8088/proxy/application_1587827373944_4203\n",
       "SparkContext available as 'sc' (version = 2.4.0-cdh6.3.2, master = yarn, app id = application_1587827373944_4203)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import scala.util.parsing.json._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.parsing.json._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INPUT: String = hdfs:///user/pknees/amazon-reviews/full/reviews_devset.json\n",
       "STOPWORDS: String = hdfs:///user/e11944050/stopwords.txt\n",
       "DELIMS: String = [.!?,;:()\\[\\]{}\\-_\"\\`~#&*%$\\/\\s\\d]\n",
       "N_DOCS_IN_CAT_KEY: String = _nDocs\n",
       "TOP_N: Int = 200\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val INPUT = \"hdfs:///user/pknees/amazon-reviews/full/reviews_devset.json\"\n",
    "val STOPWORDS = \"hdfs:///user/e11944050/stopwords.txt\"\n",
    "val DELIMS = \"[.!?,;:()\\\\[\\\\]{}\\\\-_\\\"\\\\`~#&*%$\\\\/\\\\s\\\\d]\"\n",
    "val N_DOCS_IN_CAT_KEY = \"_nDocs\"\n",
    "val TOP_N = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link reviews dataset in hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amazonReviewSet: org.apache.spark.rdd.RDD[String] = hdfs:///user/pknees/amazon-reviews/full/reviews_devset.json MapPartitionsRDD[1] at textFile at <console>:30\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val amazonReviewSet = sc.textFile(INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count reviews in different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:31: warning: non-variable type argument String in type pattern scala.collection.immutable.Map[String,String] (the underlying of Map[String,String]) is unchecked since it is eliminated by erasure\n",
       "               case Some(e:Map[String,String]) => (e(\"category\"), 1)\n",
       "                           ^\n",
       "mapDocsPerCat: (JSONString: String)(String, Int)\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapDocsPerCat(JSONString: String) : (String, Int) = {\n",
    "    val parsed = JSON.parseFull(JSONString)\n",
    "    parsed match {\n",
    "        case Some(e:Map[String,String]) => (e(\"category\"), 1)  //parse category from JSON and emit (parsedCategory, 1)\n",
    "        case _ => (\"_error\", 0)\n",
    "    }  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nDocsPerCat: scala.collection.Map[String,Int] = Map(Kindle_Store -> 3205, Electronic -> 7825, Automotive -> 1374, Pet_Supplie -> 1235, Clothing_Shoes_and_Jewelry -> 5749, Baby -> 916, Musical_Instrument -> 500, Grocery_and_Gourmet_Food -> 1297, Book -> 22507, Movies_and_TV -> 4607, Tools_and_Home_Improvement -> 1926, Sports_and_Outdoor -> 3269, CDs_and_Vinyl -> 3749, Apps_for_Android -> 2638, Home_and_Kitche -> 4254, Office_Product -> 1243, Health_and_Personal_Care -> 2982, Digital_Music -> 836, Cell_Phones_and_Accessorie -> 3447, Beauty -> 2023, Toys_and_Game -> 2253, Patio_Lawn_and_Garde -> 994)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nDocsPerCat = amazonReviewSet.map(review => mapDocsPerCat(review)) //for each review, parse category and emit 1\n",
    "                                .reduceByKey(_+_) //reduce by category and sum emitted 1s\n",
    "                                .collectAsMap() //collect results as map and keep in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link stopwords in hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopWords: org.apache.spark.rdd.RDD[String] = hdfs:///user/e11944050/stopwords.txt MapPartitionsRDD[5] at textFile at <console>:30\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopWords = sc.textFile(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect stopwords as set and keep in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopWordsSet: scala.collection.immutable.Set[String] = Set(serious, latterly, looks, particularly, used, down, regarding, entirely, it's, regardless, moreover, please, \"\", ourselves, able, that's, behind, for, despite, maybe, viz, further, corresponding, any, wherein, across, name, allows, there's, this, haven't, instead, in, ought, myself, have, your, off, once, i'll, are, is, his, oh, why, rd, knows, too, among, course, greetings, somewhat, everyone, seen, likely, said, try, already, soon, nobody, got, given, using, less, am, consider, hence, than, accordingly, isn't, four, didn't, anyhow, want, three, forth, whereby, himself, specify, yes, throughout, inasmuch, but, you're, whether, sure, below, co, best, plus, becomes, what, unto, different, would, although, elsewhere, causes, anoth..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopWordsSet = stopWords.collect().toSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse reviewTexts and categories from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:30: warning: non-variable type argument String in type pattern scala.collection.immutable.Map[String,String] (the underlying of Map[String,String]) is unchecked since it is eliminated by erasure\n",
       "               case Some(e: Map[String, String]) => {\n",
       "                            ^\n",
       "parseReviews: (JSONString: String)(String, Any)\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseReviews(JSONString: String)  = {\n",
    "    val parsed = JSON.parseFull(JSONString)\n",
    "    parsed match {\n",
    "        case Some(e: Map[String, String]) => {\n",
    "            val reviewText = e(\"reviewText\")\n",
    "            val category = e(\"category\")\n",
    "            (reviewText, category)         \n",
    "        }\n",
    "        case _ => (\"_error\", \"_error\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parsedReviews: org.apache.spark.rdd.RDD[(String, Any)] = MapPartitionsRDD[28] at map at <console>:31\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parsedReviews = amazonReviewSet.map(json => parseReviews(json))\n",
    "//containing (reviewText, category) tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tuples of (term, category), for every term. Casefold, remove stopwords and 1 char terms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "termCats: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[7] at flatMap at <console>:33\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val termCats = parsedReviews.flatMap(rc => (rc._1.split(DELIMS) //reviewCat pairs\n",
    "                             .distinct //only emit each term,cat pair once per review\n",
    "                             .map(x => x.toLowerCase) //casefolding (before stopWord filterinng)\n",
    "                             .filter(x => !stopWordsSet.contains(x)) //remove stopwords\n",
    "                             .filter(x => x.length > 1) //only keep chars with len > 1\n",
    "                             .map(x => (x, rc._2)))) //make tuple of term and category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce by terms and create maps containing document occurrences in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduceTermCats: (term: String, categories: Iterable[String])(String, Map[String,Int])\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduceTermCats(term: String, categories: Iterable[String]) : (String, Map[String, Int]) = {\n",
    "    var tempMap = categories.groupBy(identity).mapValues(_.size) //group by category and count occurrence\n",
    "    val nDocs = tempMap.foldLeft(0)(_+_._2) //calculate sum of all map values\n",
    "    tempMap += (N_DOCS_IN_CAT_KEY -> nDocs) //add total number of documents containing term to map  \n",
    "    (term, tempMap)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "termOccs: org.apache.spark.rdd.RDD[(String, Map[String,Int])] = MapPartitionsRDD[9] at map at <console>:32\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val termOccs = termCats.groupByKey()\n",
    "                            .map((tc) => reduceTermCats(tc._1, tc._2))\n",
    "//containing (term, Map[category, numOccurrences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate chi square value for every term and category combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calcChiSquare: (category: String, catOccMap: Map[String,Int])(String, BigDecimal)\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcChiSquare(category: String, catOccMap: Map[String, Int]) : (String, BigDecimal) = {\n",
    "    var temp =  catOccMap.get(category)\n",
    "    temp match {\n",
    "        case Some(e: Int) => {\n",
    "            val a = BigDecimal(e)\n",
    "            temp = catOccMap.get(N_DOCS_IN_CAT_KEY)\n",
    "            temp match {\n",
    "                case Some(e2: Int) => {\n",
    "                    val b = BigDecimal(e2).-(a)\n",
    "                    temp = nDocsPerCat.get(category)\n",
    "                    temp match {\n",
    "                        case Some(e3: Int) => {\n",
    "                            val c = BigDecimal(e3).-(a)\n",
    "                            val n = nDocsPerCat.foldLeft(0)(_+_._2) //calculate sum of all map values\n",
    "                            val d = n.-(e3).-(b)\n",
    "                            val nomChi = ((a.*(d)).-((b.*(c)))).pow(2).*(n)\n",
    "                            val denomChi = (a.+(b)).*((a.+(c))).*((b.+(d))).*((c.+(d)))\n",
    "                            val chi = nomChi./(denomChi)\n",
    "                            (category, chi)    \n",
    "                        }\n",
    "                        case _ => {(\"_error\", BigDecimal(0))}\n",
    "                    }           \n",
    "                }\n",
    "                case _ => {(\"_error\", BigDecimal(0))}\n",
    "            }\n",
    "        }\n",
    "        case _ => {(\"_error\", BigDecimal(0))}  \n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "catTermChis: org.apache.spark.rdd.RDD[(String, (String, BigDecimal))] = MapPartitionsRDD[10] at flatMap at <console>:33\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val catTermChis = termOccs.flatMap(termOcc => termOcc._2 // pairs of (term, MAP<category, occurrence>), take each MAP\n",
    "                             .filter(catOcc => !catOcc._1.equals(N_DOCS_IN_CAT_KEY)) //remove key N_DOCS_IN_CAT_KEY \n",
    "                             .map(catOcc => calcChiSquare(catOcc._1, termOcc._2)) //calcchiSquare for remaining keys \n",
    "                                                                                     //(all occurrences in categories)\n",
    "                             .map(catChi => (catChi._1, (termOcc._1, catChi._2)))) //map result to (category, (term, chi)) format\n",
    "//containing (category, (term, chiSquare))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce over category, sort by descending chi square values and only keep top 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduceCatTermChis: (category: String, termChis: Iterable[(String, BigDecimal)])(String, Array[(String, BigDecimal)])\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduceCatTermChis(category: String, termChis: Iterable[(String, BigDecimal)]) : (String, Array[(String, BigDecimal)]) = {\n",
    "    val sortedTermChis = scala.util.Sorting.stableSort(termChis.toList, \n",
    "                                                       (e1: (String, BigDecimal), e2: (String, BigDecimal)) => e1._2 > e2._2)\n",
    "    (category, sortedTermChis.take(TOP_N))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "termsPerCat: org.apache.spark.rdd.RDD[(String, Array[(String, BigDecimal)])] = ShuffledRDD[33] at sortByKey at <console>:33\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val termsPerCat = catTermChis.groupByKey()\n",
    "            .map(ctc => reduceCatTermChis(ctc._1, ctc._2))\n",
    "            .sortByKey() //sort by categories\n",
    "//containing (category, (term, chiSquare))\n",
    "//only top 200 terms per category\n",
    "//sorted by category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create string representation of termChiPairs and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stringRep: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[16] at map at <console>:29\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stringRep = termsPerCat.map(line => line._1 + \" \" + \n",
    "                                line._2.map(tuple => tuple._1 + \":\" + tuple._2)\n",
    "                                .mkString(\" \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove file from hdfs before saving new one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `a2/term_chi': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r a2/term_chi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringRep.saveAsTextFile(\"a2/term_chi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary and save it to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[26] at sortBy at <console>:32\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dict = termsPerCat.flatMap(line => line._2\n",
    "                               .map(termChi => termChi._1))\n",
    "                        .distinct\n",
    "                        .sortBy(term => term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove file from hdfs before saving new one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/05/06 14:21:50 INFO fs.TrashPolicyDefault: Moved: 'hdfs://nameservice1/user/e11944050/a2/dict' to trash at: hdfs://nameservice1/user/e11944050/.Trash/Current/user/e11944050/a2/dict\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r a2/dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict.saveAsTextFile(\"a2/dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct final output file via shell script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "! . make_output.sh a2/term_chi a2/dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (spylon-kernel)",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
