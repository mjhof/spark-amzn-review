{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIC2020 - A2.3 SparkML Pipeline to train category prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://c100.local:8088/proxy/application_1587827373944_5391\n",
       "SparkContext available as 'sc' (version = 2.4.0-cdh6.3.2, master = yarn, app id = application_1587827373944_5391)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.RegexTokenizer\n",
       "import org.apache.spark.ml.feature.StopWordsRemover\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, IDF, ChiSqSelector, StringIndexer, ChiSqSelectorModel, Normalizer}\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.ml.classification.{OneVsRest, LinearSVC}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, IDF, ChiSqSelector, StringIndexer, ChiSqSelectorModel, Normalizer}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "import org.apache.spark.ml.classification.{OneVsRest, LinearSVC}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INPUT: String = hdfs:///user/pknees/amazon-reviews/full/reviews_devset.json\n",
       "DELIMS: String = [.!?,;:()\\[\\]{}\\-_\"\\`~#&*%$\\/\\s\\d]\n",
       "STOPWORDS: String = hdfs:///user/e11944050/stopwords.txt\n",
       "TOP_N_FEATURES: Int = 4000\n",
       "RANDOM_SEED: Int = 42\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val INPUT = \"hdfs:///user/pknees/amazon-reviews/full/reviews_devset.json\"\n",
    "val DELIMS = \"[.!?,;:()\\\\[\\\\]{}\\\\-_\\\"\\\\`~#&*%$\\\\/\\\\s\\\\d]\"\n",
    "val STOPWORDS = \"hdfs:///user/e11944050/stopwords.txt\"\n",
    "val TOP_N_FEATURES = 4000\n",
    "val RANDOM_SEED = 42 //to make splits reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load JSON data from file and print schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "amazonReviewDfFull: org.apache.spark.sql.DataFrame = [asin: string, category: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val amazonReviewDfFull = spark.read.json(INPUT)\n",
    "amazonReviewDfFull.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only select reviewText and category columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "columnSelection: Seq[String] = List(reviewText, category)\n",
       "reviewCategoryDf: org.apache.spark.sql.DataFrame = [reviewText: string, category: string]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columnSelection = Seq(\"reviewText\", \"category\")\n",
    "val reviewCategoryDf = amazonReviewDfFull.select(columnSelection.map(c => col(c)): _*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopWords: Array[String] = Array(a's, able, about, above, according, accordingly, across, actually, after, afterwards, again, against, ain't, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, appear, appreciate, appropriate, are, aren't, around, as, aside, ask, asking, associated, at, available, away, awfully, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, both, brief, but, by, c'mon, c's, came, can, can't, cannot, cant, cause, causes, certain, certainly, changes, clearly, co, com, come, comes, concerning, consequently, consider, considering, conta..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopWords = spark.read.textFile(STOPWORDS).as[String].collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regexTokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_3401ac26f392\n",
       "remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_b515e2736645\n",
       "countVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_2785c6a41ed0\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_2c7d0ae1851a\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_e2c02fcc571d\n",
       "selector: org.apache.spark.ml.feature.ChiSqSelector = chiSqSelector_a72fde60fa69\n",
       "normalizer: org.apache.spark.ml.feature.Normalizer = normalizer_bfa5c3b1614f\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val regexTokenizer = new RegexTokenizer()\n",
    "                .setInputCol(\"reviewText\")\n",
    "                .setOutputCol(\"terms\")\n",
    "                .setPattern(DELIMS)\n",
    "                .setToLowercase(true)\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "              .setInputCol(\"terms\")\n",
    "              .setOutputCol(\"termsFiltered\")\n",
    "              .setStopWords(stopWords)\n",
    "\n",
    "val countVectorizer = new CountVectorizer() \n",
    "                    .setInputCol(\"termsFiltered\")\n",
    "                    .setOutputCol(\"rawFeatures\") \n",
    "                    .setMinDF(100) //otherwise ChiSqSelection takes too much time\n",
    "\n",
    "val idf = new IDF()\n",
    "        .setInputCol(\"rawFeatures\")\n",
    "        .setOutputCol(\"idfFeatures\")\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    "            .setInputCol(\"category\")\n",
    "            .setOutputCol(\"label\")\n",
    "\n",
    "val selector = new ChiSqSelector()\n",
    "               .setNumTopFeatures(TOP_N_FEATURES)\n",
    "               .setFeaturesCol(\"idfFeatures\")\n",
    "               .setLabelCol(\"label\")\n",
    "               .setOutputCol(\"selectedFeatures\")\n",
    "\n",
    "val normalizer = new Normalizer()\n",
    "                  .setInputCol(\"selectedFeatures\")\n",
    "                  .setOutputCol(\"features\")\n",
    "                  .setP(2.0) //L2 Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit preprocessing pipeline and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prpPipeline: org.apache.spark.ml.Pipeline = pipeline_1d117ce2781c\n",
       "prpModel: org.apache.spark.ml.PipelineModel = pipeline_1d117ce2781c\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prpPipeline = new Pipeline().setStages(Array(\n",
    "                                regexTokenizer, \n",
    "                                remover, \n",
    "                                countVectorizer, \n",
    "                                idf, \n",
    "                                indexer, \n",
    "                                selector,\n",
    "                                normalizer))\n",
    "\n",
    "val prpModel = prpPipeline.fit(reviewCategoryDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prpData: org.apache.spark.sql.DataFrame = [reviewText: string, category: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prpData = prpModel.transform(reviewCategoryDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [reviewText: string, category: string ... 7 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [reviewText: string, category: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train, test) = prpData.randomSplit(Array(0.8, 0.2), seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classifier pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier: org.apache.spark.ml.classification.LinearSVC = linearsvc_2db3ee075a5b\n",
       "ovr: org.apache.spark.ml.classification.OneVsRest = oneVsRest_750e29cf94ce\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinearsvc_2db3ee075a5b-maxIter: 5,\n",
       "\tlinearsvc_2db3ee075a5b-regParam: 0.01,\n",
       "\tlinearsvc_2db3ee075a5b-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_2db3ee075a5b-maxIter: 5,\n",
       "\tlinearsvc_2db3ee075a5b-regParam: 0.1,\n",
       "\tlinearsvc_2db3ee075a5b-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_2db3ee075a5b-maxIter: 5,\n",
       "\tlinearsvc_2db3ee075a5b-regParam: 0.2,\n",
       "\tlinearsvc_2db3ee075a5b-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_2db3ee075a5b-maxIter: 5,\n",
       "\tlinearsvc_2db3ee075a5b-regParam: 0.01,\n",
       "\tlinearsvc_2db3ee075a5b-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_2db3ee075a5b-maxIter: 5,\n",
       "\tlinearsvc_2db3ee075a5b-regParam: ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// instantiate the base classifier\n",
    "val classifier = new LinearSVC()\n",
    "                \n",
    "// instantiate the One Vs Rest Classifier.\n",
    "val ovr = new OneVsRest().setClassifier(classifier)\n",
    "\n",
    "// We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "// TrainValidationSplit will try all combinations of values and determine best model using\n",
    "// the evaluator.\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "                .addGrid(classifier.regParam, Array(0.01, 0.1, 0.2)) //try 3 settings for regularization \n",
    "                .addGrid(classifier.maxIter, Array(5, 10)) //try 2 settings for max iterations\n",
    "                .addGrid(classifier.standardization, Array(true, false)) //try with and without standardization\n",
    "                .build()\n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "                            .setEstimator(ovr)\n",
    "                            .setEvaluator(new MulticlassClassificationEvaluator().setMetricName(\"f1\"))\n",
    "                            .setEstimatorParamMaps(paramGrid) // 80% of the data will be used for training and the remaining 20% for validation.\n",
    "                            .setTrainRatio(0.8) // Evaluate up to 2 parameter settings in parallel\n",
    "                            .setParallelism(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model pipeline on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionModel: org.apache.spark.ml.tuning.TrainValidationSplitModel = tvs_e73d8808c5aa\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionModel = trainValidationSplit.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform test data and show some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(3608,[0,1,3,4,8,...|  0.0|       0.0|\n",
      "|(3608,[0,1,2,5,10...|  0.0|       0.0|\n",
      "|(3608,[0,1,2,5,16...|  0.0|       0.0|\n",
      "|(3608,[0,1,2,5,16...|  0.0|       0.0|\n",
      "|(3608,[0,1,2,4,8,...|  0.0|       0.0|\n",
      "|(3608,[0,1,2,4,5,...|  0.0|       0.0|\n",
      "|(3608,[0,1,2,5,6,...|  0.0|       0.0|\n",
      "|(3608,[1,3,5,14,3...|  0.0|       0.0|\n",
      "|(3608,[0,1,2,4,23...|  0.0|       0.0|\n",
      "|(3608,[0,1,2,4,5,...|  0.0|       0.0|\n",
      "|(3608,[0,2,22,47,...|  0.0|       0.0|\n",
      "|(3608,[1,2,12,14,...|  0.0|       0.0|\n",
      "|(3608,[0,2,4,8,54...|  0.0|       0.0|\n",
      "|(3608,[0,1,2,6,8,...|  0.0|       0.0|\n",
      "|(3608,[0,1,2,5,8,...|  0.0|       0.0|\n",
      "|(3608,[0,1,2,3,6,...|  0.0|       0.0|\n",
      "|(3608,[0,1,2,5,7,...|  0.0|       0.0|\n",
      "|(3608,[0,1,2,5,7,...|  0.0|       0.0|\n",
      "|(3608,[0,1,4,189,...| 18.0|      12.0|\n",
      "|(3608,[1,67,358,5...| 10.0|      10.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [reviewText: string, category: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = predictionModel.transform(test)\n",
    "\n",
    "predictions.select(\"features\", \"label\", \"prediction\")\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions over test data with F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_6b5479353acb\n",
       "f1: Double = 0.6562831535787289\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator().setMetricName(\"f1\")\n",
    "val f1 = evaluator.evaluate(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (spylon-kernel)",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
